{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gqimax.mapper import cuda_map_cx\n",
    "import cupy as cp\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "def map_cx(lambdass, indicess, control, target):\n",
    "    \"\"\"\n",
    "    --- First, I encode the n-qubit Pauli word as list of n int8 array\n",
    "    Ex: XYI --> [1, 2, 0]\n",
    "    For n-stabilizer, we have n x k words, so the encoded tensor will be n x k x n (ragged tensor)\n",
    "    --- Next step, I flatten this tensor to 1D array (each element is still n-dim array)\n",
    "    Flatten vector: [[0 1 2]\n",
    "    [0 2 2]\n",
    "    [0 3 2]\n",
    "    [1 2 3]\n",
    "    [2 3 1]\n",
    "    [2 2 1]]\n",
    "    --- Map this array to the new array using map_cx kernel\n",
    "    Mapped flatten vector: [[0 1 2]\n",
    "    [3 2 2]\n",
    "    [3 3 2]\n",
    "    [2 3 3]\n",
    "    [1 2 1]\n",
    "    [1 3 1]]\n",
    "    Lambdas: [ 1  1  1  1  1 -1]\n",
    "    \n",
    "    --- Finally, I unflatten the mapped array to the original shape (ragged tensor)\n",
    "    --- Obviously, this function requires starts variable (the start index of each row in the flatten vector)\n",
    "    \n",
    "    Out ragged tensor (with starts = [0,3,4]): [array([[0, 1, 2],\n",
    "        [3, 2, 2],\n",
    "        [3, 3, 2]], dtype=int8), array([[2, 3, 3]], dtype=int8), array([[1, 2, 1],\n",
    "        [1, 3, 1]], dtype=int8)]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def flatten_ragged_matrix_cupy(ragged_matrix):\n",
    "        lengths = cp.array([len(row) for row in ragged_matrix], dtype=cp.int8)\n",
    "        starts = cp.concatenate((cp.array([0]), cp.cumsum(lengths)), dtype=cp.int8)\n",
    "        flatten_vector = cp.concatenate(ragged_matrix, dtype=cp.int8)\n",
    "        return flatten_vector, starts[:-1]\n",
    "\n",
    "    flatten_indicess, starts = flatten_ragged_matrix_cupy(indicess)\n",
    "\n",
    "    flatten_lambdas_sign, mapped_flatten_indicess = cuda_map_cx(flatten_indicess, control, target)\n",
    "    print(\"mapped_flatten_indicess\", mapped_flatten_indicess)\n",
    "    print(\"flatten_lambdas_sign\", flatten_lambdas_sign)\n",
    "    \n",
    "    starts = starts[1:].tolist()\n",
    "\t# Convert flatten vector to ragged tensor\n",
    "    ragged_indicess = cp.vsplit(mapped_flatten_indicess, starts)\n",
    "    ragged_lambdas_sign = cp.split(flatten_lambdas_sign, starts)\n",
    "\t# OP: lambdas_sign * ragged_lambdas\n",
    "    # This operator can be implemented in CUDA kernel (in file notebook)\n",
    "    # But I see there is no different between two methods\n",
    "    print(ragged_lambdas_sign)\n",
    "    print(lambdass)\n",
    "    return [cp.multiply(m1, m2) for m1, m2 in zip(lambdass, ragged_lambdas_sign)], ragged_indicess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import cupy as cp\n",
    "\n",
    "lambdass = [\n",
    "\tcp.array([ 9.65925820e-01,  2.58819093e-01, -2.18556924e-08]), \n",
    " \tcp.array([1.])\n",
    "]\n",
    "\n",
    "indicess = [\n",
    "    cp.array([[1, 1],[2, 1],[3, 0]]), \n",
    "    cp.array([[3, 3]], dtype = cp.int8)\n",
    "]\n",
    "map_cx(lambdass, indicess, 0, 1)\n",
    "\n",
    "# [0.9659]*xi + [0.2588]*yi\n",
    "# [1]*iz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
