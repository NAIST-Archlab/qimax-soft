{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Benchmark hàm NumPy\u001b[39;00m\n\u001b[0;32m     69\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 70\u001b[0m result_numpy \u001b[38;5;241m=\u001b[39m \u001b[43mweightsss_to_lambdas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweightss_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m numpy_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThời gian NumPy (CPU): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumpy_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m giây\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m, in \u001b[0;36mweightsss_to_lambdas\u001b[1;34m(lambdass, weightsss)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m streams[i]:\n\u001b[1;32m---> 44\u001b[0m         new_lambdas, non_zeros_indices \u001b[38;5;241m=\u001b[39m \u001b[43mweightss_to_lambda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlambdass\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweightsss\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m         mapped_lambdass[i] \u001b[38;5;241m=\u001b[39m new_lambdas\n\u001b[0;32m     48\u001b[0m         non_zeros_indicess[i] \u001b[38;5;241m=\u001b[39m non_zeros_indices\n",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m, in \u001b[0;36mweightss_to_lambda\u001b[1;34m(lambdas, weightss)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweightss_to_lambda\u001b[39m(lambdas: cp\u001b[38;5;241m.\u001b[39mndarray, weightss: cp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m cp\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A sum of transformed word (a matrix k x n x 4) to list\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    Example for a single transformed word (treated as 1 term): \u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m        lambda*[[1,2,3,4], [1,2,3,4]] \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m        np.ndarray: lambdas\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     num_terms, num_qubits, _ \u001b[38;5;241m=\u001b[39m weightss\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     19\u001b[0m     new_lambdas \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnum_qubits)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_terms):\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import cupy as cp\n",
    "from gqimax.mapper import map_indices_to_indicess\n",
    "def weightss_to_lambda(lambdas: cp.ndarray, weightss: cp.ndarray) -> cp.ndarray:\n",
    "    \"\"\"A sum of transformed word (a matrix k x n x 4) to list\n",
    "    Example for a single transformed word (treated as 1 term): \n",
    "        lambda*[[1,2,3,4], [1,2,3,4]] \n",
    "            -> lambda*[1, 2, 3, 4, 2, 4, 6, 8, 3, 6, 9, 12, 4, 8, 12, 16]\n",
    "    Example for this function (sum, 2 qubits, 3 term):\n",
    "        [[[1,2,3,4], [1,2,3,4]], [[1,2,3,4], [1,2,3,4]], [[1,2,3,4], [1,2,3,4]]] \n",
    "            -> [ 3.  6.  9. 12.  6. 12. 18. 24.  9. 18. 27. 36. 12. 24. 36. 48.]\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: lambdas\n",
    "    \"\"\"\n",
    "    num_terms, num_qubits, _ = weightss.shape\n",
    "    new_lambdas = cp.zeros(4**num_qubits)\n",
    "    for j in range(num_terms):\n",
    "        weights = weightss[j]\n",
    "        products = weights[0]\n",
    "        for k in range(1, num_qubits):\n",
    "            products = cp.outer(products, weights[k]).ravel()\n",
    "        new_lambdas += lambdas[j] * products\n",
    "    # This lambdas is still in the form of 4^n x 1, \n",
    "    # we need to ignore 0 values in the next steps\n",
    "    # In the worst case, there is no 0 values.\n",
    "    non_zeros_indices = cp.nonzero(new_lambdas)[0]\n",
    "    new_lambdas = new_lambdas[non_zeros_indices]\n",
    "    return new_lambdas, non_zeros_indices\n",
    "\n",
    "\n",
    "\n",
    "def weightsss_to_lambdas(lambdass: list, weightsss: list) -> cp.ndarray:\n",
    "    n = len(weightsss)\n",
    "    streams = [cp.cuda.Stream() for _ in range(n)]\n",
    "    mapped_lambdass = [None] * n\n",
    "    non_zeros_indicess = [None] * n\n",
    "    \n",
    "    # Gán công việc cho từng stream\n",
    "    for i in range(n):\n",
    "        with streams[i]:\n",
    "            new_lambdas, non_zeros_indices = weightss_to_lambda(\n",
    "                lambdass[i], weightsss[i]\n",
    "            )\n",
    "            mapped_lambdass[i] = new_lambdas\n",
    "            non_zeros_indicess[i] = non_zeros_indices\n",
    "    \n",
    "    # Đợi tất cả stream hoàn thành\n",
    "    for stream in streams:\n",
    "        stream.synchronize()\n",
    "    \n",
    "    return mapped_lambdass, map_indices_to_indicess(non_zeros_indicess)\n",
    "\n",
    "# Thiết lập tham số\n",
    "num_qubits = 10  # 4^10 = 1,048,576 phần tử\n",
    "num_terms = 200\n",
    "\n",
    "# Tạo dữ liệu ngẫu nhiên cho NumPy\n",
    "weightss_np = np.random.rand(num_qubits, num_terms, num_qubits, 4)\n",
    "lambdas_np = np.random.rand(num_qubits, num_terms)\n",
    "\n",
    "# Chuyển dữ liệu sang CuPy\n",
    "weightss_cp = cp.asarray(weightss_np)\n",
    "lambdas_cp = cp.asarray(lambdas_np)\n",
    "\n",
    "# Benchmark hàm NumPy\n",
    "start_time = time.time()\n",
    "result_numpy = weightsss_to_lambdas(weightss_np, lambdas_np)\n",
    "numpy_time = time.time() - start_time\n",
    "print(f\"Thời gian NumPy (CPU): {numpy_time:.4f} giây\")\n",
    "\n",
    "\n",
    "\n",
    "# if np.allclose(result_numpy, result_cupy):\n",
    "#     print(\"Kết quả giống nhau.\")\n",
    "# else:\n",
    "#     print(\"Kết quả khác nhau.\")\n",
    "\n",
    "# # Tính tốc độ tăng tốc\n",
    "# speedup = numpy_time / cupy_time\n",
    "# print(f\"Tăng tốc: {speedup:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 4) (2,)\n",
      "(1, 3, 4) (1,)\n",
      "(1, 3, 4) (1,)\n"
     ]
    }
   ],
   "source": [
    "from gqimax.mapper import weightsss_to_lambdas\n",
    "num_qubits = 3  # 4^10 = 1,048,576 phần tử\n",
    "num_terms = 5\n",
    "ks = [np.random.randint(1, num_terms) for _ in range(num_qubits)]\n",
    "\n",
    "weightsss_np = [cp.random.rand(ks[i], num_qubits, 4) * (cp.random.rand(ks[i], num_qubits, 4) > 0.5) for i in range(num_qubits)]\n",
    "lambdass_np = [cp.random.rand(ks[i]) for i in range(num_qubits)]\n",
    "for i in range(num_qubits):\n",
    "\tprint(weightsss_np[i].shape, lambdass_np[i].shape)\n",
    "mapped_lambdass, non_zeros_indicess = weightsss_to_lambdas(weightsss_np, lambdass_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambdass[0]: [0.00144713 0.00110885 0.00139623 0.00053744]\n",
      "non_zeros_indicess: [ 2  6 10 14]\n",
      "lambdass[1]: [0.17136025 0.3747416  0.18828798 0.41176026 0.09784203 0.21396724]\n",
      "non_zeros_indicess: [ 9 11 41 43 57 59]\n",
      "lambdass[2]: [0.0037968  0.00178773 0.00043901 0.00839594 0.00395323 0.00097079\n",
      " 0.02371438 0.01116593 0.002742   0.05244004 0.02469142 0.00606344]\n",
      "non_zeros_indicess: [ 8  9 11 12 13 15 56 57 59 60 61 63]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_qubits):\n",
    "\tprint(f\"lambdass[{i}]: {mapped_lambdass[i]}\")\n",
    "\tprint(f\"non_zeros_indicess: {non_zeros_indicess[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hàm wrapper để gọi kernel\n",
    "def cuda_map_cx(words_array, control, target):\n",
    "    \"\"\"\n",
    "    Áp dụng map_cx trên mảng k word bằng CUDA kernel.\n",
    "    Args:\n",
    "        words_array: cp.ndarray shape (k, n), dtype=cp.int8\n",
    "        control: int, chỉ số control\n",
    "        target: int, chỉ số target\n",
    "    Returns:\n",
    "        lambdas: cp.ndarray shape (k,), dtype=cp.int8\n",
    "        new_words_array: cp.ndarray shape (k, n), dtype=cp.int8\n",
    "    \"\"\"\n",
    "    k, n = words_array.shape\n",
    "    new_words_array = cp.empty_like(words_array, dtype=cp.int8)\n",
    "    lambdas = cp.empty(k, dtype=cp.int8)\n",
    "\n",
    "    block_size = 256\n",
    "    grid_size = (k + block_size - 1) // block_size\n",
    "\n",
    "    map_cx_kernel((grid_size,), (block_size,), \n",
    "                  (words_array, new_words_array, lambdas, k, n, control, target))\n",
    "\n",
    "    return lambdas, new_words_array\n",
    "\n",
    "def flatten_ragged_matrix_cupy(ragged_matrix):\n",
    "    lengths = cp.array([len(row) for row in ragged_matrix], dtype=cp.int8)\n",
    "    starts = cp.concatenate((cp.array([0]), cp.cumsum(lengths)), dtype=cp.int8)\n",
    "    flatten_vector = cp.concatenate(ragged_matrix, dtype=cp.int8)\n",
    "    return flatten_vector, starts[:-1]\n",
    "\n",
    "def unflatten_ragged_matrix_cupy(flatten_vector, starts):\n",
    "    return cp.vsplit(flatten_vector, starts[1:].tolist())\n",
    "\n",
    "def map_indices_to_weighted(ragged_lambdas, ragged_tensor, control, target):\n",
    "    \"\"\"\n",
    "    --- First, I encode the n-qubit Pauli word (index) as list of n int8 array\n",
    "    Ex: 0(III) --> [0, 0, 0]\n",
    "    For n-stabilizer, we have n x k indices, so the encoded tensor will be n x k x n (ragged tensor)\n",
    "    Original tensor: [\n",
    "\t\tarray([1, 2]),\n",
    "\t\tarray([3]),\n",
    "\t\tarray([4]),\n",
    "\t]\n",
    "    --- Next step, I flatten this tensor to 1D array (each element is still n-dim array)\n",
    "    Flatten vector: [1,2,3,4] and following starts (variable) = [0, 2, 3]\n",
    "    --- Map this array to the new array using map_indices_to_weighted kernel\n",
    "    Mapped flatten vector: [\n",
    "        [0, 0, 1],\n",
    "        [0, 0, 2],\n",
    "        [0, 0, 3],\n",
    "        [0, 1, 0],\n",
    "    ] (n x k x n)\n",
    "    \n",
    "    --- Finally, I unflatten the mapped array to the original shape (ragged tensor)\n",
    "    --- Obviously, this function requires starts variable (the start index of each row in the flatten vector)\n",
    "    \n",
    "    Out ragged tensor (with starts = [0, 2, 3]): [\n",
    "        array([[0, 0, 1], [0, 0, 2]]),\n",
    "\t\tarray([[0, 0, 3]]),\n",
    "\t\tarray([[0, 1, 0]]),\n",
    "    ]\n",
    "\n",
    "    \"\"\"\n",
    "    flatten_vector, starts = flatten_ragged_matrix_cupy(ragged_tensor)\n",
    "    lambdas_sign, mapped_flatten_vector = cuda_map_cx(flatten_vector, control, target)\n",
    "    starts = starts[1:].tolist()\n",
    "\t# Convert flatten vector to ragged tensor\n",
    "    ragged_tensor = cp.vsplit(mapped_flatten_vector, starts)\n",
    "    lambdas_sign = cp.split(lambdas_sign, starts)\n",
    "\t# OP: lambdas_sign * ragged_lambdas\n",
    "    # This operator can be implemented in CUDA kernel (in file notebook)\n",
    "    # But I see there is no different between two methods\n",
    "    return [cp.multiply(m1, m2) for m1, m2 in zip(ragged_lambdas, lambdas_sign)], ragged_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2,  6, 10, 14,  9, 11, 41, 43, 57, 59,  8,  9, 11, 12, 13, 15, 56,\n",
       "        57, 59, 60, 61, 63], dtype=int8),\n",
       " array([ 0,  4, 10], dtype=int8))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_ragged_matrix_cupy(non_zeros_indicess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 2,  6, 10, 14], dtype=int64),\n",
       " array([ 9, 11, 41, 43, 57, 59], dtype=int64),\n",
       " array([ 8,  9, 11, 12, 13, 15, 56, 57, 59, 60, 61, 63], dtype=int64)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_zeros_indicess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightsss = cp.array([\n",
    "    [[0,0, 1,4], [1,2,3,4]], \n",
    "    [[0,1,0,0], [1,2,0,0]], \n",
    "    [[1,0,0,4], [0,0, 1,4]]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.5873697 ,  2.34947881,  0.69422608,  1.38845216,  0.58182811,\n",
       "         1.16365622,  1.74548433,  2.32731244,  2.32731244,  4.65462487,\n",
       "         9.33141613, 18.70716501]),\n",
       " array([ 2,  3,  4,  5,  8,  9, 10, 11, 12, 13, 14, 15], dtype=int64))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from gqimax.mapper import weightss_to_lambda\n",
    "from gqimax.utils import index_to_indices, index_to_word\n",
    "\n",
    "lambdas = weightss_to_lambda(weightsss, lambdass_np[0])\n",
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0, 0],\n",
      "       [0, 1],\n",
      "       [0, 2],\n",
      "       [0, 3],\n",
      "       [1, 0],\n",
      "       [1, 1],\n",
      "       [1, 2],\n",
      "       [1, 3],\n",
      "       [2, 0],\n",
      "       [2, 1],\n",
      "       [2, 2],\n",
      "       [2, 3]]), array([[0, 0],\n",
      "       [0, 1],\n",
      "       [0, 2],\n",
      "       [0, 3],\n",
      "       [1, 0],\n",
      "       [1, 1],\n",
      "       [1, 2],\n",
      "       [1, 3],\n",
      "       [2, 0],\n",
      "       [2, 1],\n",
      "       [2, 2],\n",
      "       [2, 3]])]\n"
     ]
    }
   ],
   "source": [
    "indicess = []\n",
    "for i in range(2):\t\n",
    "    indices = cp.nonzero(lambdas)[0]\n",
    "    lambdas = lambdas[indices]\n",
    "    new_indices = []\n",
    "    for index in indices:\n",
    "        new_indices.append(index_to_indices(int(index), 2))\n",
    "    indicess.append(cp.array(new_indices))\n",
    "print(indicess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
