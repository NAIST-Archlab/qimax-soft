{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total execution time for 10 jobs: 1.9461 seconds\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import time\n",
    "\n",
    "# Pre-compiled kernel (same as provided)\n",
    "kernel_code = \"\"\"\n",
    "extern \"C\" __global__\n",
    "void broadcasted_multiplies_kernel(\n",
    "    const float* lambdas,     \n",
    "    const int* shapes,        \n",
    "    const float* data,        \n",
    "    const int* offsets,       \n",
    "    const int* offsets_result, \n",
    "    float* result,             \n",
    "    int N,                     \n",
    "    int dims,                  \n",
    "    int total_size             \n",
    ") {\n",
    "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (idx >= total_size) return;\n",
    "\n",
    "    int i = 0;\n",
    "    while (i < N - 1 && idx >= offsets_result[i + 1]) i++;\n",
    "    int local_idx = idx - offsets_result[i];\n",
    "\n",
    "    int temp = local_idx;\n",
    "    int indices[4]; \n",
    "    for (int j = dims - 1; j >= 0; j--) {\n",
    "        int s = shapes[i * dims + j];\n",
    "        indices[j] = temp % s;\n",
    "        temp /= s;\n",
    "    }\n",
    "\n",
    "    float val = lambdas[i];\n",
    "    for (int j = 0; j < dims; j++) {\n",
    "        int offset = offsets[i * dims + j];\n",
    "        int index = indices[j];\n",
    "        val *= data[offset + index];\n",
    "    }\n",
    "    result[idx] = val;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Compile the kernel only once.\n",
    "broadcasted_multiplies_kernel = cp.RawKernel(kernel_code, 'broadcasted_multiplies_kernel')\n",
    "\n",
    "def broadcasted_multiplies_cuda(lambdas, shapes, data, offsets, offsets_result):\n",
    "    \"\"\"\n",
    "    Run a single broadcasted multiplication job.\n",
    "    \n",
    "    Parameters:\n",
    "      - lambdas (cp.ndarray, shape (N,)): lambda coefficients\n",
    "      - shapes (cp.ndarray, shape (N*dims,)): shape sizes for each batch item\n",
    "      - data (cp.ndarray): concatenated data arrays\n",
    "      - offsets (cp.ndarray, shape (N*dims,)): starting offsets of each array in data\n",
    "      - offsets_result (cp.ndarray, shape (N,)): starting offsets for each result batch\n",
    "      \n",
    "    Returns:\n",
    "      - result_cp (cp.ndarray): flattened result on the GPU\n",
    "      - offsets_result (cp.ndarray): offsets defining the boundaries of each batch result\n",
    "    \"\"\"\n",
    "    N = lambdas.size\n",
    "    dims = shapes.size // N\n",
    "    # Compute total output size based on last batch item.\n",
    "    total_size = int(offsets_result[-1] + cp.prod(shapes[-dims:]))\n",
    "\n",
    "    result_cp = cp.zeros(total_size, dtype=cp.float32)\n",
    "    block_size = 256\n",
    "    grid_size = (total_size + block_size - 1) // block_size\n",
    "\n",
    "    broadcasted_multiplies_kernel(\n",
    "        (grid_size,), (block_size,),\n",
    "        (lambdas, shapes, data, offsets, offsets_result, result_cp, N, dims, total_size)\n",
    "    )\n",
    "\n",
    "    return result_cp, offsets_result\n",
    "\n",
    "def prepare_data_from_arrayss(arrayss):\n",
    "    \"\"\"\n",
    "    Prepare a single \"job\" for broadcasted multiplication.\n",
    "    \n",
    "    arrayss: list of lists of cp.ndarray for one job.\n",
    "             Each inner list represents a batch item with a fixed number (dims) arrays.\n",
    "             \n",
    "    Returns:\n",
    "      A tuple (lambdas, shapes, data, offsets, offsets_result) ready for the CUDA kernel.\n",
    "      Note: This function assumes that you already have a corresponding lambdas array.\n",
    "    \"\"\"\n",
    "    k = len(arrayss)        # number of batch items for this job\n",
    "    dims = len(arrayss[0])   # number of arrays per batch item\n",
    "\n",
    "    # Build the shapes vector\n",
    "    shapes = cp.array([arr.size for arrays in arrayss for arr in arrays], dtype=cp.int32)\n",
    "\n",
    "    # Concatenate the actual data from all arrays.\n",
    "    data = cp.concatenate([arr for arrays in arrayss for arr in arrays])\n",
    "\n",
    "    # Calculate the offset of each array in the concatenated data array.\n",
    "    lengths = shapes.reshape(k, dims)\n",
    "    # Compute cumulative sum on flattened array (except the last element).\n",
    "    offsets = cp.cumsum(cp.concatenate([cp.array([0], dtype=cp.int32), lengths.flatten()[:-1]]))\n",
    "\n",
    "    # Compute the total number of elements for each batch item.\n",
    "    sizes = cp.prod(lengths.reshape(k, dims), axis=1)\n",
    "    offsets_result = cp.cumsum(cp.concatenate([cp.array([0], dtype=cp.int32), sizes[:-1]]))\n",
    "    return shapes, data, offsets, offsets_result\n",
    "\n",
    "def run_batch_broadcasted_multiplies_cuda_optimized(lambdas_list, arrayss_list):\n",
    "    \"\"\"\n",
    "    Optimally execute multiple broadcasted multiplication jobs concurrently.\n",
    "    \n",
    "    Parameters:\n",
    "      - lambdas_list: List of cp.ndarray, one per job. Each array shape: (N,)\n",
    "      - arrayss_list: List where each element is a list (length N) of lists (length dims) \n",
    "                      of cp.ndarray representing the data arrays for that job.\n",
    "                      \n",
    "    Returns:\n",
    "      - results: List of lists containing the split output result for each job.\n",
    "    \"\"\"\n",
    "    if len(lambdas_list) != len(arrayss_list):\n",
    "        raise ValueError(\"Mismatched number of lambdas and arrayss jobs.\")\n",
    "    \n",
    "    n_jobs = len(lambdas_list)\n",
    "    streams = [cp.cuda.Stream(non_blocking=True) for _ in range(n_jobs)]\n",
    "    results = [None] * n_jobs\n",
    "\n",
    "    # Launch each job asynchronously in its own stream.\n",
    "    for i, (lambdas_job, arrayss) in enumerate(zip(lambdas_list, arrayss_list)):\n",
    "        # Create (or ensure) lambdas_job is on GPU in float32 format.\n",
    "        lambdas_job = lambdas_job.astype(cp.float32) if lambdas_job.dtype != cp.float32 else lambdas_job\n",
    "        \n",
    "        with streams[i]:\n",
    "            # Prepare job-specific data.\n",
    "            shapes, data, offsets, offsets_result = prepare_data_from_arrayss(arrayss)\n",
    "            \n",
    "            # Note: If your `prepare_data_from_arrayss` should include lambdas, modify accordingly.\n",
    "            # Here we assume lambdas_job is already prepared (and must have the same number of elements as arrayss).\n",
    "            if lambdas_job.size != len(arrayss):\n",
    "                raise ValueError(\"The size of lambdas does not match the number of batch items in arrayss.\")\n",
    "            \n",
    "            result_cp, offsets_result = broadcasted_multiplies_cuda(\n",
    "                lambdas_job, shapes, data, offsets, offsets_result\n",
    "            )\n",
    "            \n",
    "            # Split the flattened result into separate arrays using the precomputed offsets.\n",
    "            results[i] = cp.split(result_cp, offsets_result[1:].tolist())\n",
    "\n",
    "    # Wait for all streams to finish execution.\n",
    "    for stream in streams:\n",
    "        stream.synchronize()\n",
    "\n",
    "    return results\n",
    "\n",
    "##########################################\n",
    "# Example usage of the optimized version #\n",
    "##########################################\n",
    "\n",
    "# Let's say we want to run 10 jobs.\n",
    "n_jobs = 10\n",
    "\n",
    "# Prepare a list of lambdas arrays.\n",
    "lambdas_list = [cp.array([1] * 50000, dtype=cp.float32) for _ in range(n_jobs)]\n",
    "\n",
    "# Prepare a list of arrayss; each job has 5000 batch items and each batch item contains 4 arrays.\n",
    "arrayss_list = [\n",
    "    [\n",
    "        [cp.array([2, 1, 1], dtype=cp.float32),\n",
    "         cp.array([1.2], dtype=cp.float32),\n",
    "         cp.array([1, 2], dtype=cp.float32),\n",
    "         cp.array([3, 4], dtype=cp.float32)]\n",
    "        for _ in range(50000)\n",
    "    ]\n",
    "    for _ in range(n_jobs)\n",
    "]\n",
    "\n",
    "# Time the overall execution.\n",
    "start = time.time()\n",
    "all_results = run_batch_broadcasted_multiplies_cuda_optimized(lambdas_list, arrayss_list)\n",
    "cp.cuda.Stream.null.synchronize()  # Ensure that all asynchronous work is complete.\n",
    "print(\"Total execution time for {} jobs: {:.4f} seconds\".format(n_jobs, time.time() - start))\n",
    "\n",
    "# Now, all_results[i] holds the list of the per-batch result arrays for job i.\n",
    "# for res in all_results:\n",
    "#     print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
